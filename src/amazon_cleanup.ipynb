{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/apb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/apb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/apb/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/apb/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "Use `LIMIT` to control the upper limit of objects from each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Files: 3, Combined lines: 30000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIMIT = 10000\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "    if i == LIMIT:\n",
    "      break\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "original_reviews = Path(\"../data/original\")\n",
    "reviews = [getDF(path) for path in original_reviews.iterdir() if path.is_file()]\n",
    "df = pd.concat(reviews)\n",
    "f\"Files: {len(reviews)}, Combined lines: {len(df)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# lowercase review text\n",
    "df[\"reviewText\"] = df[\"reviewText\"].str.lower()\n",
    "# remove unverified\n",
    "df = df[df[\"verified\"] == True]\n",
    "# remove unixReviewTime\n",
    "df.drop([\"unixReviewTime\",\"reviewerID\", \"image\", \"style\", \"asin\"], axis=1, inplace=True)\n",
    "\n",
    "# rename overall to rating\n",
    "df.rename(columns = {\"overall\": \"rating\"}, inplace=True)\n",
    "\n",
    "# set votes containing NaN to 0\n",
    "df[\"vote\"] = df[\"vote\"].fillna(0)\n",
    "# overall to int from float\n",
    "df[\"rating\"] = df[\"rating\"].astype(int)\n",
    "# remove NaN reviewText\n",
    "df[\"reviewText\"] = df[\"reviewText\"].fillna(\"\")\n",
    "\n",
    "# remove stop words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "df[\"reviewText\"] = df[\"reviewText\"].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# remove punctuation from reviewText\n",
    "# [^\\w\\s]' -> looks for anything that isnt a word or whitespace to remove\n",
    "df[\"reviewText\"] = df[\"reviewText\"].str.replace('[^\\w\\s]',\"\")\n",
    "\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# WordNetLemmatizer.lemmatize() only lemmatizes based on tag-parameter given, e.g. \"v\" for verb, \"n\" for noun\n",
    "# This method tries to determine the right tag automatically\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "\n",
    "df[\"reviewText\"] = df[\"reviewText\"].apply(lemmatize_text)\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "df.to_csv(\"../data/cleaned_reviews.tsv\", index=False, sep=\"\\t\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
